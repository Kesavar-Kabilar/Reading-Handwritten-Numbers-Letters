{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a keras sequential model to interpret any single digit number or a letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "44RUqTby_1m4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates the mapping dictionary to map from the number \n",
    "to the corresponding letter/digit. \n",
    "\n",
    "Note: there are not 62 unique labels because some letters \n",
    "like c, i, j, k, l, m, o, p, s, u, v, w, x, y, z have \n",
    "very similar lowercase and uppercase letters. \n",
    "Hence there are 46 labels\n",
    "\"\"\"\n",
    "\n",
    "mapping_file = open(\"EMNIST/emnist-bymerge-mapping.txt\")\n",
    "\n",
    "mapping = {}\n",
    "for line in mapping_file:\n",
    "    line = line.split(\" \")\n",
    "    mapping[line[0]] = chr(int(line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting the information frome mnist-digits-train.csv file, \n",
    "emnist-digits-test.csv and storing the contents in Xtrain, \n",
    "Ttrain (Used to train the model), and Xtest, \n",
    "Ttest (Used to test the model).\n",
    "\n",
    "Xtrain, Xtest contains rows of an array with 784. Which represents \n",
    "an 28 by 28 pixel image and the values represent the greyscale / 255.\n",
    "Xtrain contains 697 931 rows while Xtest contains 116 322 rows.\n",
    "\n",
    "Ttrain, Ttest contains a number from 0 to 9 which corresponds \n",
    "to the handwritten digit for their repesctive row of numbers.\n",
    "\"\"\"\n",
    "\n",
    "Xtrain = pd.read_csv(\"EMNIST/emnist-bymerge-train.csv\", dtype=\"int16\")\n",
    "Xtest = pd.read_csv(\"EMNIST/emnist-bymerge-test.csv\", dtype=\"int16\")\n",
    "\n",
    "Ttrain = Xtrain[\"24\"]\n",
    "Ttest = Xtest[\"24\"]\n",
    "\n",
    "Xtrain.drop(columns=[\"24\"], inplace=True)\n",
    "Xtest.drop(columns=[\"24\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, Compiling, Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses keras to create a keras sequential model with 4 layers. \n",
    "First layer takes the input which corresponds to our data. \n",
    "The second, and third layer is the hidden layer. The dense represents that \n",
    "this layer is fully connected to the previous layer. It also uses \n",
    "the rectify linear unit activation function. The fourth layer \n",
    "represents all the possible outputs which corresponds to \n",
    "the possible digits and letters in our data set. It also uses the \n",
    "softmax activation function.\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(784,1)), \n",
    "    keras.layers.Dense(392, activation=\"relu\"),\n",
    "    keras.layers.Dense(112, activation=\"relu\"),\n",
    "    keras.layers.Dense(47, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compiles the model with adam (Adaptive Moment Estimation) \n",
    "optimizer, cross entropy loss function, and we want to \n",
    "focus on accuracy.\n",
    "\"\"\"\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "698/698 [==============================] - 18s 23ms/step - loss: 3.2292 - accuracy: 0.3819\n",
      "Epoch 2/12\n",
      "698/698 [==============================] - 17s 25ms/step - loss: 0.9543 - accuracy: 0.7130\n",
      "Epoch 3/12\n",
      "698/698 [==============================] - 18s 25ms/step - loss: 0.6317 - accuracy: 0.8031\n",
      "Epoch 4/12\n",
      "698/698 [==============================] - 20s 29ms/step - loss: 0.5289 - accuracy: 0.83000s - l\n",
      "Epoch 5/12\n",
      "698/698 [==============================] - 22s 31ms/step - loss: 0.4809 - accuracy: 0.8429\n",
      "Epoch 6/12\n",
      "698/698 [==============================] - 23s 32ms/step - loss: 0.4507 - accuracy: 0.85091s - loss: 0.4508 - accura - ETA: 0s - loss: 0\n",
      "Epoch 7/12\n",
      "698/698 [==============================] - 20s 29ms/step - loss: 0.4292 - accuracy: 0.8564\n",
      "Epoch 8/12\n",
      "698/698 [==============================] - 18s 26ms/step - loss: 0.4143 - accuracy: 0.8604\n",
      "Epoch 9/12\n",
      "698/698 [==============================] - 18s 26ms/step - loss: 0.4000 - accuracy: 0.8647\n",
      "Epoch 10/12\n",
      "698/698 [==============================] - 21s 30ms/step - loss: 0.3905 - accuracy: 0.86700s - loss: 0.3903 - accu\n",
      "Epoch 11/12\n",
      "698/698 [==============================] - 20s 28ms/step - loss: 0.3804 - accuracy: 0.8695\n",
      "Epoch 12/12\n",
      "698/698 [==============================] - 21s 30ms/step - loss: 0.3743 - accuracy: 0.8707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12a734b8b20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains the model with the given Xtrain and Ttrain\n",
    "with the batch size of 1000 and epochs of 12.\n",
    "\"\"\"\n",
    "\n",
    "model.fit(Xtrain, Ttrain, batch_size=1000, epochs=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3636/3636 [==============================] - 8s 2ms/step - loss: 0.4232 - accuracy: 0.8590\n",
      "Test accuracy: 0.859046459197998\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(Xtest, Ttest, verbose=1) \n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input with Test Data\n",
    "### The user can input any number from 0 to 39 998 (Inclusive) and the function will output the image of the number, the correct number that is being represented, and the model's predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(image_num):\n",
    "    \"\"\"\n",
    "    Takes an input from 0, 116 321 and uses the validation data \n",
    "    from the data set and will print the image, actual number \n",
    "    and the predicted number by the model.\n",
    "    \"\"\"\n",
    "    image = np.array(Xtest.loc[image_num])\n",
    "    \n",
    "    # Predicts the model \n",
    "    prediction = model.predict(image.reshape(1, 784))\n",
    "\n",
    "    image = image.reshape((28, 28))\n",
    "\n",
    "    plt.imshow(image.T)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Actual Result: \" + mapping[str(Ttest.loc[image_num])])\n",
    "\n",
    "    predicted_num = np.argmax(prediction[0])\n",
    "    print(\"Model Predicted Number: \" + mapping[str(predicted_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Result: 7\n",
      "Model Predicted Number: 7\n"
     ]
    }
   ],
   "source": [
    "test_data(116321)\n",
    "# Input any value between 0 and 116 321 inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input with a Created Image\n",
    "### The user can alter the number.png and the model will attempt to accurately decipher the written number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_written_num(filename):\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((28, 28))\n",
    "    # Converts the image to a greyscall image\n",
    "    img = np.asarray(img)\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.dot(img[:28, :28, :3], [0.2989, 0.5870, 0.1140])\n",
    "    \n",
    "    # Displays the image\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    # Reshapes the image to the correct shape to use in the model\n",
    "    img = img.T\n",
    "    img = img.reshape((1, 784))\n",
    "    \n",
    "    predictions = model.predict([img])\n",
    "    predicted_num = np.argmax(predictions[0])\n",
    "    print(\"Model Predicted Number: \" + mapping[str(predicted_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predicted Number: N\n"
     ]
    }
   ],
   "source": [
    "read_written_num(\"picture.png\")\n",
    "# Attempts to read the number written in number.png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x12a6e509be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save a Model (1st way)\n",
    "\n",
    "This method will save\n",
    "    Architecture of the model, allowing to recreate the model\n",
    "    The weights of the model\n",
    "    Training configuartions (loss, optimizer)\n",
    "    State of the optimizer, allowing to resume training exactly where you left off.\n",
    "\"\"\"\n",
    "model.save(\"Model.h5\")\n",
    "\n",
    "\"\"\"\n",
    "Loads a saved model\n",
    "\"\"\"\n",
    "model = keras.models.load_model(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save a Model's Architecture\n",
    "\"\"\"\n",
    "json_string = model.to_json()\n",
    "# yaml_string = model.to_yaml()\n",
    "\"\"\"\n",
    "Loads the Model's Architecture\n",
    "\"\"\"\n",
    "model_architecture = keras.models.model_from_json(json_string)\n",
    "# model_architecture = keras.models.model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save a Model's Weights\n",
    "\"\"\"\n",
    "model.save_weights(\"Model_Weights.h5\")\n",
    "\"\"\"\n",
    "Loads the Model's Weights\n",
    "\"\"\"\n",
    "model2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(784,1)), \n",
    "    keras.layers.Dense(392, activation=\"relu\"),\n",
    "    keras.layers.Dense(112, activation=\"relu\"),\n",
    "    keras.layers.Dense(47, activation=\"softmax\")\n",
    "])\n",
    "model2.load_weights(\"Model_Weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HandWrittenDigits.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
